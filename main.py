#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Scraper OLX - Otimizado (Fast Version)
"""

import sys
import re
import argparse
import logging
import os
import time
from datetime import datetime
from typing import Dict, Optional
import requests
from bs4 import BeautifulSoup

# Selenium imports
SELENIUM_AVAILABLE = False
try:
    from selenium import webdriver
    from selenium.webdriver.chrome.service import Service
    from selenium.webdriver.chrome.options import Options
    from selenium.webdriver.common.by import By
    from selenium.webdriver.support.ui import WebDriverWait
    from selenium.webdriver.support import expected_conditions as EC
    from selenium.common.exceptions import TimeoutException, NoSuchElementException
    from webdriver_manager.chrome import ChromeDriverManager
    SELENIUM_AVAILABLE = True
except ImportError:
    SELENIUM_AVAILABLE = False


def setup_logging():
    log_dir = 'logs'
    if not os.path.exists(log_dir):
        os.makedirs(log_dir)
    log_filename = os.path.join(log_dir, f'olx_scraper_{datetime.now().strftime("%Y%m%d")}.log')
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(levelname)s - %(message)s',
        handlers=[logging.FileHandler(log_filename, encoding='utf-8'), logging.StreamHandler(sys.stdout)]
    )
    return logging.getLogger(__name__)

logger = setup_logging()

def setup_selenium_driver() -> Optional[webdriver.Chrome]:
    if not SELENIUM_AVAILABLE:
        return None
    
    try:
        chrome_options = Options()
        chrome_options.add_argument('--headless')
        chrome_options.add_argument('--no-sandbox')
        chrome_options.add_argument('--disable-dev-shm-usage')
        chrome_options.add_argument('--disable-gpu')
        chrome_options.add_argument('--window-size=1920,1080')
        # User agent comum para evitar bloqueios simples
        chrome_options.add_argument('--user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36')
        
        # --- OTIMIZA√á√ÉO 1: ESTRAT√âGIA DE CARREGAMENTO ---
        # 'eager': O DOMContentLoaded disparou? Libera o script. N√£o espera imagens/css/ads.
        chrome_options.page_load_strategy = 'eager' 
        
        # --- OTIMIZA√á√ÉO 2: BLOQUEAR IMAGENS E CSS ---
        prefs = {
            "profile.managed_default_content_settings.images": 2,  # Bloqueia imagens
            "profile.default_content_setting_values.notifications": 2,
            "profile.managed_default_content_settings.stylesheets": 2,
            "profile.managed_default_content_settings.cookies": 1,
            "profile.managed_default_content_settings.javascript": 1,
            "profile.managed_default_content_settings.plugins": 1,
            "profile.managed_default_content_settings.popups": 2,
            "profile.managed_default_content_settings.geolocation": 2,
            "profile.managed_default_content_settings.media_stream": 2,
        }
        chrome_options.add_experimental_option("prefs", prefs)
        
        service = Service(ChromeDriverManager().install())
        driver = webdriver.Chrome(service=service, options=chrome_options)
        
        # Timeout reduzido para ser mais √°gil na falha
        driver.implicitly_wait(2) 
        
        return driver
    except Exception as e:
        logger.error(f"Erro driver: {e}")
        return None

def clean_text(text):
    if not text: return None
    # Limpeza geral r√°pida
    text = re.sub(r'[\n\r\t]', ' ', text)
    return re.sub(r'\s+', ' ', text).strip()

def extract_data_selenium(url: str) -> Dict[str, Optional[str]]:
    driver = None
    data = {
        'id_anuncio': None,  # ID extra√≠do da URL
        'nome_vendedor': None, 
        'marca_veiculo': None,
        'modelo_veiculo': None,  # Modelo extra√≠do da URL
        'versao_veiculo': None,  # Vers√£o extra√≠da do H1
        'valor_anuncio': None, 
        'preco_fipe': None,
        'telefone': None,
        'quilometragem': None,
        'bairro': None,
        'cidade_estado_cep': None,
        'ano_veiculo': None,
        'preco_medio_olx': None,
        'link': None  # Link do an√∫ncio no formato https://olx.com.br/vi/{id}
    }
    
    # Extra√ß√£o do ID do an√∫ncio e MARCA da URL principal
    # Padr√£o: N√∫mero grande (8-10 d√≠gitos) ap√≥s h√≠fen, pode estar no final ou antes de ? ou #
    # Exemplos:
    # .../mini-cooper-1-6-impecavel-1460372718
    # .../fiat-uno-mille-1-0-fire-f-flex-economy-2p-2002-1460309392?rec=h...
    # Remove query string e fragmento para buscar apenas no path
    url_path = url.split('?')[0].split('#')[0]
    # Busca n√∫meros de 8-10 d√≠gitos ap√≥s h√≠fen (para evitar pegar anos como 2002 que tem 4 d√≠gitos)
    # Pode estar no final ou antes de ? ou #
    id_match = re.search(r'-(\d{8,10})(?:[?/#]|$)', url_path)
    if id_match:
        data['id_anuncio'] = id_match.group(1)
        logger.debug(f"ID do an√∫ncio extra√≠do da URL: {data['id_anuncio']}")
    else:
        # Fallback: busca n√∫meros de 6+ d√≠gitos (caso o ID tenha formato diferente)
        id_match = re.search(r'-(\d{6,})(?:[?/#]|$)', url_path)
        if id_match:
            data['id_anuncio'] = id_match.group(1)
            logger.debug(f"ID do an√∫ncio extra√≠do da URL (fallback): {data['id_anuncio']}")
    
    # Constru√ß√£o do link no formato https://olx.com.br/vi/{id}
    if data['id_anuncio']:
        data['link'] = f"https://olx.com.br/vi/{data['id_anuncio']}"
        logger.debug(f"Link constru√≠do: {data['link']}")
    
    # Extra√ß√£o da MARCA da URL principal (se dispon√≠vel)
    # Padr√£o: .../autos-e-pecas/carros-vans-e-utilitarios/MARCA/...
    marca_url_match = re.search(r'/autos-e-pecas/carros-vans-e-utilitarios/([^/]+)/', url_path)
    if marca_url_match:
        marca_url = marca_url_match.group(1)
        marcas_validas = ['volkswagen', 'vw', 'fiat', 'chevrolet', 'ford', 'toyota', 'honda', 'hyundai', 'renault', 'peugeot', 'citroen', 'nissan', 'mitsubishi', 'suzuki', 'kia', 'jeep', 'ram', 'dodge', 'bmw', 'mercedes', 'audi', 'mini']
        if marca_url.lower() in [m.lower() for m in marcas_validas] or any(m.lower() in marca_url.lower() for m in marcas_validas):
            data['marca_veiculo'] = marca_url.replace('-', ' ').title()
            logger.debug(f"Marca encontrada na URL principal: {data['marca_veiculo']} (de: {marca_url})")
    
    try:
        logger.info("Iniciando Selenium (Modo Turbo)...")
        driver = setup_selenium_driver()
        if not driver: return data

        driver.get(url)
        
        # Espera expl√≠cita curta apenas pelo elemento principal (H1 ou Pre√ßo)
        try:
            WebDriverWait(driver, 5).until(
                EC.presence_of_element_located((By.TAG_NAME, "h1"))
            )
        except TimeoutException:
            logger.warning("Tempo limite aguardando H1, tentando extrair mesmo assim...")

        # --- ESTRAT√âGIA R√ÅPIDA: PARSE VIA BEAUTIFULSOUP ---
        # Em vez de pedir pro Selenium buscar elemento por elemento (lento),
        # pegamos o HTML atual e processamos com BS4 (instant√¢neo).
        html = driver.page_source
        soup = BeautifulSoup(html, 'lxml')

        # 1. Extra√ß√£o PRE√áO (Prioridade: span.typo-title-large)
        price_elem = soup.select_one('span.typo-title-large')
        if price_elem:
            val = clean_text(price_elem.get_text())
            # Regex para garantir que pegamos s√≥ o dinheiro
            match = re.search(r'R\$\s*[\d.,]+', val)
            if match: 
                data['valor_anuncio'] = match.group(0)
                logger.debug(f"Pre√ßo encontrado com seletor span.typo-title-large: {data['valor_anuncio']}")
        
        # Fallback: Tenta outros seletores se o principal n√£o funcionar
        if not data['valor_anuncio']:
            price_elem = soup.find('h2', string=re.compile(r'R\$\s*[\d.,]+')) 
            if not price_elem:
                price_elem = soup.select_one('h2.ad__sc-1leoitd-0, h2[class*="price"]')
            
            if price_elem:
                val = clean_text(price_elem.get_text())
                match = re.search(r'R\$\s*[\d.,]+', val)
                if match: 
                    data['valor_anuncio'] = match.group(0)
                    logger.debug(f"Pre√ßo encontrado com fallback: {data['valor_anuncio']}")

        # 2. Extra√ß√£o VERS√ÉO (Prioridade H1 ou campos espec√≠ficos)
        # O H1 no OLX geralmente √© "Vers√£o do Carro - Ano"
        h1 = soup.find('h1')
        if h1:
            title = clean_text(h1.get_text())
            # Remove sufixos comuns do OLX no t√≠tulo para limpar a vers√£o
            title = re.sub(r'\s*-\s*\d+\s*\|\s*OLX.*', '', title, flags=re.I)
            title = re.sub(r'\s*\|\s*OLX.*', '', title, flags=re.I)
            data['versao_veiculo'] = title
        
        if not data['versao_veiculo']:
            # Fallback: Procura label "Modelo" ou "Vers√£o"
            model_label = soup.find(string=re.compile(r'^Modelo$', re.I))
            if model_label:
                # Tenta o pr√≥ximo link ou span
                next_elem = model_label.find_next(['a', 'span', 'p'])
                if next_elem: data['versao_veiculo'] = clean_text(next_elem.get_text())

        # 3. Extra√ß√£o VENDEDOR
        # Procura por "Nome | √öltimo acesso" ou classes de perfil
        vendor_elem = soup.select_one('span.typo-body-large.ad__sc-ypp2u2-4')
        if vendor_elem:
            data['nome_vendedor'] = clean_text(vendor_elem.get_text())
        else:
            # Busca gen√©rica por conteiner de perfil
            profile_box = soup.select_one('div[data-testid="account-box"], .ad__sc-ypp2u2-12')
            if profile_box:
                # O nome geralmente √© o primeiro texto forte ou span grande
                txt = profile_box.get_text(separator='|', strip=True)
                parts = txt.split('|')
                if parts:
                    # Filtra lixo como "√öltimo acesso"
                    nome = parts[0]
                    if "acesso" not in nome.lower() and len(nome) > 1:
                        data['nome_vendedor'] = nome

        # 4. Extra√ß√£o FIPE e PRE√áO M√âDIO OLX
        # Busca pelos containers LkJa2kno e identifica pelo label dentro de cada um
        
        # Busca todos os containers com a classe LkJa2kno
        containers = soup.select('div.LkJa2kno')
        logger.debug(f"Containers LkJa2kno encontrados: {len(containers)}")
        
        for container in containers:
            # Busca o label dentro do container (span com data-variant="overline")
            label_elem = container.find('span', {'data-variant': 'overline'})
            if not label_elem:
                continue
            
            label_text = clean_text(label_elem.get_text()).upper()
            
            # Busca o pre√ßo dentro do container (span com as classes espec√≠ficas)
            preco_elem = container.select_one('span[data-ds-component="DS-Text"].olx-text.olx-text--body-medium.olx-text--block.olx-text--bold')
            if not preco_elem:
                continue
            
            preco_text = clean_text(preco_elem.get_text())
            preco_match = re.search(r'R\$\s*[\d.,]+', preco_text)
            if not preco_match:
                continue
            
            preco_value = preco_match.group(0)
            
            # Identifica se √© FIPE ou Pre√ßo M√©dio OLX pelo label
            if 'FIPE' in label_text and not data['preco_fipe']:
                data['preco_fipe'] = preco_value
                logger.debug(f"Pre√ßo FIPE encontrado: {data['preco_fipe']}")
            elif ('M√âDIO' in label_text or 'MEDIO' in label_text) and 'OLX' in label_text and not data['preco_medio_olx']:
                data['preco_medio_olx'] = preco_value
                logger.debug(f"Pre√ßo M√©dio OLX encontrado: {data['preco_medio_olx']}")
            
            # Se j√° encontrou ambos, pode parar
            if data['preco_fipe'] and data['preco_medio_olx']:
                break

        # 5. Extra√ß√£o TELEFONE
        # Seletor espec√≠fico: span.ad__sc-14mcmsd-7.hORwFH ou span.typo-body-large.font-light.ad__sc-14mcmsd-7
        telefone_elem = soup.select_one('span.ad__sc-14mcmsd-7.hORwFH, span.typo-body-large.font-light.ad__sc-14mcmsd-7')
        if telefone_elem:
            telefone_text = clean_text(telefone_elem.get_text())
            telefone_match = re.search(r'\(?\d{2}\)?\s*\d{4,5}-?\d{4}', telefone_text)
            if telefone_match:
                data['telefone'] = telefone_match.group(0)
                logger.debug(f"Telefone encontrado: {data['telefone']}")
        
        # Fallback: Tenta o seletor anterior se o espec√≠fico n√£o funcionar
        if not data['telefone']:
            telefone_elem = soup.select_one('span.typo-body-large.text-neutral-120')
            if telefone_elem:
                telefone_text = clean_text(telefone_elem.get_text())
                telefone_match = re.search(r'\(?\d{2}\)?\s*\d{4,5}-?\d{4}', telefone_text)
                if telefone_match:
                    data['telefone'] = telefone_match.group(0)
                    logger.debug(f"Telefone encontrado (fallback): {data['telefone']}")

        # 6. Extra√ß√£o QUILOMETRAGEM
        km_elems = soup.select('span.ad__sc-hj0yqs-0.ekhFnR')
        logger.debug(f"Elementos de quilometragem encontrados: {len(km_elems)}")
        for km_elem in km_elems:
            # Pega apenas o texto direto do span (antes dos divs internos)
            # O valor est√° no texto principal do span, n√£o nos divs filhos
            km_text = ''
            
            # M√©todo 1: Itera pelos conte√∫dos diretos e pega apenas strings (texto direto)
            for content in km_elem.contents:
                if isinstance(content, str):
                    km_text += content.strip()
            
            # M√©todo 2: Se n√£o encontrou texto direto, pega o texto e filtra
            if not km_text or not km_text.strip():
                # Pega todo o texto do elemento
                km_text = km_elem.get_text(separator='', strip=True)
                # Remove tudo que n√£o seja n√∫mero (limpa divs internos e espa√ßos)
                km_text = re.sub(r'[^\d]', '', km_text)
            
            # Extrai o primeiro n√∫mero encontrado (o valor principal)
            if km_text and km_text.strip():
                km_match = re.search(r'\d+', km_text)
                if km_match:
                    km_value = km_match.group(0)
                    # Valida se √© um n√∫mero razo√°vel de quilometragem (entre 0 e 9999999)
                    if km_value.isdigit() and 0 <= int(km_value) <= 9999999:
                        data['quilometragem'] = km_value
                        logger.debug(f"Quilometragem encontrada: {data['quilometragem']}")
                        break  # Para no primeiro elemento v√°lido encontrado
        
        # Fallback: Se n√£o encontrou, busca por texto "Quilometragem" e pega o pr√≥ximo elemento
        if not data['quilometragem']:
            quilometragem_label = soup.find(string=re.compile(r'Quilometragem', re.I))
            if quilometragem_label:
                parent = quilometragem_label.find_parent()
                if parent:
                    # Procura o span com a classe espec√≠fica no mesmo container
                    km_elem = parent.find('span', class_=re.compile(r'ad__sc-hj0yqs-0|ekhFnR'))
                    if km_elem:
                        km_text = ''
                        for content in km_elem.contents:
                            if isinstance(content, str):
                                km_text += content.strip()
                        if not km_text:
                            km_text = re.sub(r'[^\d]', '', km_elem.get_text(separator='', strip=True))
                        if km_text:
                            km_match = re.search(r'\d+', km_text)
                            if km_match:
                                km_value = km_match.group(0)
                                if km_value.isdigit() and 0 <= int(km_value) <= 9999999:
                                    data['quilometragem'] = km_value
                                    logger.debug(f"Quilometragem encontrada (fallback): {data['quilometragem']}")

        # 7. Extra√ß√£o BAIRRO
        bairro_elems = soup.select('span.typo-body-medium.font-semibold')
        for elem in bairro_elems:
            # Verifica se n√£o √© um link e n√£o cont√©m marca/ano
            if elem.name != 'a':
                bairro_text = clean_text(elem.get_text())
                # Filtra: n√£o deve ser num√©rico de 4 d√≠gitos (ano) nem marca conhecida
                if bairro_text and not re.match(r'^\d{4}$', bairro_text) and len(bairro_text) > 5:
                    # Verifica se n√£o √© marca conhecida (pode ser expandido)
                    marcas_conhecidas = ['volkswagen', 'fiat', 'chevrolet', 'ford', 'toyota', 'honda', 'hyundai', 'renault', 'peugeot', 'citroen']
                    if not any(marca.lower() in bairro_text.lower() for marca in marcas_conhecidas):
                        data['bairro'] = bairro_text
                        logger.debug(f"Bairro encontrado: {data['bairro']}")
                        break

        # 8. Extra√ß√£o CIDADE/ESTADO/CEP
        local_elem = soup.select_one('span.typo-body-small.font-semibold.text-neutral-110')
        if local_elem:
            local_text = clean_text(local_elem.get_text())
            if local_text:
                data['cidade_estado_cep'] = local_text
                logger.debug(f"Cidade/Estado/CEP encontrado: {data['cidade_estado_cep']}")

        # 9. Extra√ß√£o ANO, MARCA e MODELO (usam o mesmo seletor, diferenciar por conte√∫do e URL)
        ano_marca_elems = soup.select('a.ad__sc-2h9gkk-3.lkkHCr')
        logger.debug(f"Elementos a.ad__sc-2h9gkk-3.lkkHCr encontrados: {len(ano_marca_elems)}")
        
        for elem in ano_marca_elems:
            text = clean_text(elem.get_text())
            href = elem.get('href', '')
            
            if text:
                # Verifica se √© ano (4 d√≠gitos num√©ricos)
                if re.match(r'^\d{4}$', text):
                    if not data['ano_veiculo']:
                        data['ano_veiculo'] = text
                        logger.debug(f"Ano encontrado: {data['ano_veiculo']}")
                # Se n√£o √© ano e n√£o √© muito curto, pode ser marca
                elif len(text) > 2 and not re.match(r'^\d+$', text):
                    # Verifica se n√£o √© vers√£o (geralmente vers√µes s√£o mais longas e espec√≠ficas)
                    if not data['marca_veiculo'] and len(text) < 20:
                        # Lista b√°sica de marcas conhecidas para valida√ß√£o
                        marcas_validas = ['volkswagen', 'vw', 'fiat', 'chevrolet', 'ford', 'toyota', 'honda', 'hyundai', 'renault', 'peugeot', 'citroen', 'nissan', 'mitsubishi', 'suzuki', 'kia', 'jeep', 'ram', 'dodge', 'bmw', 'mercedes', 'audi', 'mini']
                        # Verifica se o texto cont√©m ou √© uma marca conhecida
                        text_lower = text.lower()
                        for marca in marcas_validas:
                            if marca.lower() == text_lower or marca.lower() in text_lower or text_lower in marca.lower():
                                data['marca_veiculo'] = text
                                logger.debug(f"Marca encontrada no texto: {data['marca_veiculo']}")
                                break
            
            # Extra√ß√£o da MARCA e MODELO da URL
            # Padr√£o da URL: .../autos-e-pecas/carros-vans-e-utilitarios/MARCA/MODELO/...
            if href:
                # Procura o padr√£o na URL: /autos-e-pecas/carros-vans-e-utilitarios/MARCA/MODELO/
                url_match = re.search(r'/autos-e-pecas/carros-vans-e-utilitarios/([^/]+)/([^/]+)/', href)
                if url_match:
                    marca_url = url_match.group(1)
                    modelo_url = url_match.group(2)
                    
                    # Extra√ß√£o da MARCA da URL (se ainda n√£o foi encontrada)
                    if not data['marca_veiculo'] and marca_url:
                        # Lista de marcas conhecidas para validar
                        marcas_validas = ['volkswagen', 'vw', 'fiat', 'chevrolet', 'ford', 'toyota', 'honda', 'hyundai', 'renault', 'peugeot', 'citroen', 'nissan', 'mitsubishi', 'suzuki', 'kia', 'jeep', 'ram', 'dodge', 'bmw', 'mercedes', 'audi', 'mini']
                        if marca_url.lower() in [m.lower() for m in marcas_validas] or any(m.lower() in marca_url.lower() for m in marcas_validas):
                            data['marca_veiculo'] = marca_url.replace('-', ' ').title()
                            logger.debug(f"Marca encontrada na URL: {data['marca_veiculo']} (de: {marca_url})")
                    
                    # Extra√ß√£o do MODELO da URL
                    # Lista de segmentos que N√ÉO s√£o modelo (estados, regi√µes, etc.)
                    segmentos_excluidos = [
                        'estado-sp', 'estado-pr', 'estado-rj', 'estado-mg', 'estado-sc', 'estado-rs', 
                        'estado-ba', 'estado-go', 'estado-pe', 'estado-ce', 'estado-df', 'estado-es',
                        'estado-ma', 'estado-ms', 'estado-mt', 'estado-pa', 'estado-pb', 'estado-pi',
                        'regiao-de-sorocaba', 'regiao', 'sao-paulo-e-regiao', 'zona-leste', 'zona-norte',
                        'zona-sul', 'zona-oeste', 'centro', 'grande-sao-paulo', 'abc'
                    ]
                    # Verifica se o modelo n√£o √© um segmento exclu√≠do e ainda n√£o foi encontrado
                    if not data['modelo_veiculo'] and modelo_url and modelo_url.lower() not in [s.lower() for s in segmentos_excluidos]:
                        # Formata o modelo: substitui h√≠fens por espa√ßos e capitaliza
                        data['modelo_veiculo'] = modelo_url.replace('-', ' ').title()
                        logger.debug(f"Modelo encontrado na URL: {data['modelo_veiculo']} (de: {modelo_url} em href: {href[:80]})")

        logger.info(f"Dados extra√≠dos: {data}")
        return data

    except Exception as e:
        logger.error(f"Erro Selenium: {e}")
        return data
    finally:
        if driver:
            try:
                # No modo eager, o quit √†s vezes trava se a p√°gina ainda estiver carregando scripts
                # For√ßamos o fechamento.
                driver.quit()
            except:
                pass

def main():
    parser = argparse.ArgumentParser(description='OLX Scraper Fast')
    parser.add_argument('url', nargs='?', help='URL do an√∫ncio')
    args = parser.parse_args()
    
    url = args.url
    if not url:
        # Se n√£o tem argumento, pede input (modo interativo)
        print("="*60)
        print("üîç SCRAPER OLX - Extra√ß√£o de Dados de An√∫ncios")
        print("="*60)
        print()
        print("üìã Cole a URL do an√∫ncio do OLX e pressione Enter:")
        url = input().strip()
        
        # Remove caracteres invis√≠veis e espa√ßos extras
        url = re.sub(r'[\s\u200b\u200c\u200d\ufeff]+', '', url)
        
        if not url:
            print("\n‚ùå Erro: URL n√£o fornecida.")
            return

    # Valida√ß√£o melhorada da URL
    url_pattern = re.compile(
        r'^https?://'  # http:// ou https://
        r'(?:[a-z0-9-]+\.)?'  # subdom√≠nio opcional (sp., www., etc.)
        r'olx\.com\.br'  # dom√≠nio olx.com.br
        r'.*',  # resto da URL
        re.IGNORECASE
    )
    
    if not url_pattern.match(url):
        print(f"\n‚ùå Erro: URL inv√°lida ou n√£o √© do OLX.")
        print(f"   URL recebida: {url[:80]}...")
        print("   A URL deve come√ßar com https://www.olx.com.br ou https://sp.olx.com.br")
        return

    print("üöÄ Iniciando extra√ß√£o r√°pida...")
    start_time = time.time()
    
    data = extract_data_selenium(url)
    
    end_time = time.time()
    
    print("\n" + "="*40)
    print(f"üÜî ID:       {data['id_anuncio'] or 'N√£o encontrado'}")
    print(f"üè≠ Marca:    {data['marca_veiculo'] or 'N√£o encontrado'}")
    print(f"üöó Modelo:   {data['modelo_veiculo'] or 'N√£o encontrado'}")
    print(f"üìã Vers√£o:   {data['versao_veiculo'] or 'N√£o encontrado'}")
    print(f"üìÖ Ano:      {data['ano_veiculo'] or 'N√£o encontrado'}")
    print(f"üìè KM:       {data['quilometragem'] or 'N√£o encontrado'}")
    print(f"üí∞ Anunciado: {data['valor_anuncio'] or 'N√£o encontrado'}")
    print(f"üìä FIPE:     {data['preco_fipe'] or 'N√£o encontrado'}")
    print(f"üìà M√©dio:    {data['preco_medio_olx'] or 'N√£o encontrado'}")
    print(f"üë§ Vendedor: {data['nome_vendedor'] or 'N√£o encontrado'}")
    print(f"üìû Telefone:  {data['telefone'] or 'N√£o encontrado'}")
    print(f"üìç Bairro:   {data['bairro'] or 'N√£o encontrado'}")
    print(f"üåç Local:    {data['cidade_estado_cep'] or 'N√£o encontrado'}")
    print(f"üîó Link:     {data['link'] or 'N√£o encontrado'}")
    print("="*40)
    print(f"‚è±Ô∏è  Tempo total: {end_time - start_time:.2f} segundos")

if __name__ == '__main__':
    main()